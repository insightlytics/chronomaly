"""
Anomaly Detection: SQLite â†’ Detect with Transformers â†’ SQLite

This example demonstrates a complete anomaly detection pipeline with transformers:
1. Read forecast data from SQLite (from previous forecast workflow)
2. Read actual data from SQLite
3. Apply transformers at different stages
4. Detect anomalies with configurable confidence intervals
5. Filter and format results
6. Write anomalies to SQLite

Use Case: Production anomaly detection with data quality filters and formatting

Requirements:
    pip install pandas

Usage:
    python examples/anomaly/sqlite_anomaly_with_transformers.py
"""

import pandas as pd
import sqlite3
from datetime import datetime, timedelta

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ANOMALY DETECTION: SQLite â†’ Transformers â†’ Detect â†’ SQLite       â•‘
â•‘                                                                      â•‘
â•‘  Pipeline: Forecast DB + Actual DB â†’ Filter â†’ Detect â†’ Format       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# ============================================================================
# STEP 1: Create sample databases
# ============================================================================

print("\nðŸ“Š STEP 1: Create sample forecast and actual data")
print("=" * 70)

# Create forecast database (simulating previous forecast workflow output)
forecast_data = pd.DataFrame({
    'date': [datetime(2024, 11, 10)] * 5,
    'platform': ['desktop', 'mobile', 'tablet', 'desktop', 'mobile'],
    'channel': ['organic', 'paid', 'organic', 'paid', 'organic'],
    'page': ['home', 'product', 'blog', 'product', 'home'],
    'sessions': [
        '5000|4500|4600|4750|4875|5000|5125|5250|5400|5500',  # desktop_organic_home
        '3000|2700|2775|2850|2925|3000|3075|3150|3240|3300',  # mobile_paid_product
        '100|90|92|95|97|100|102|105|108|110',                # tablet_organic_blog (low traffic)
        '2000|1800|1850|1900|1950|2000|2050|2100|2160|2200',  # desktop_paid_product
        '4000|3600|3700|3800|3900|4000|4100|4200|4320|4400',  # mobile_organic_home
    ]
})

# Save to SQLite
conn = sqlite3.connect('/tmp/forecasts.db')
forecast_data.to_sql('forecasts', conn, if_exists='replace', index=False)
conn.close()
print(f"âœ“ Created forecast database: /tmp/forecasts.db")
print(f"  {len(forecast_data)} forecast rows")
print()

# Create actual data with anomalies
actual_data = pd.DataFrame({
    'date': [datetime(2024, 11, 10)] * 5,
    'platform': ['desktop', 'mobile', 'tablet', 'desktop', 'mobile'],
    'channel': ['organic', 'paid', 'organic', 'paid', 'organic'],
    'page': ['home', 'product', 'blog', 'product', 'home'],
    'sessions': [
        5100,   # desktop_organic_home: IN_RANGE
        3500,   # mobile_paid_product: ABOVE_UPPER (16.67% deviation) âš ï¸
        95,     # tablet_organic_blog: IN_RANGE (but low traffic, will be filtered)
        1700,   # desktop_paid_product: BELOW_LOWER (15% deviation) âš ï¸
        3200,   # mobile_organic_home: BELOW_LOWER (20% deviation) âš ï¸
    ]
})

conn = sqlite3.connect('/tmp/actuals.db')
actual_data.to_sql('actuals', conn, if_exists='replace', index=False)
conn.close()
print(f"âœ“ Created actual database: /tmp/actuals.db")
print(f"  {len(actual_data)} actual rows")
print()

# ============================================================================
# STEP 2: Workflow configuration
# ============================================================================

print("\nðŸ”§ STEP 2: Configure workflow with transformers")
print("=" * 70)

workflow_code = '''
from chronomaly.application.workflows import AnomalyDetectionWorkflow
from chronomaly.infrastructure.data.readers.databases import SQLiteDataReader
from chronomaly.infrastructure.data.writers.databases import SQLiteDataWriter
from chronomaly.infrastructure.anomaly_detectors import ForecastActualAnomalyDetector
from chronomaly.infrastructure.transformers import DataTransformer

# Import unified transformers
from chronomaly.infrastructure.transformers.filters import (
    ValueFilter,
    CumulativeThresholdFilter
)
from chronomaly.infrastructure.transformers.formatters import ColumnFormatter

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# READERS: SQLite configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Forecast reader
forecast_reader = SQLiteDataReader(
    db_path='/tmp/forecasts.db',
    query="""
        SELECT date, platform, channel, page, sessions
        FROM forecasts
        WHERE date = '2024-11-10'
        ORDER BY date
    """
)

# Actual reader
actual_reader = SQLiteDataReader(
    db_path='/tmp/actuals.db',
    query="""
        SELECT date, platform, channel, page, sessions
        FROM actuals
        WHERE date = '2024-11-10'
        ORDER BY date
    """
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORMER: Pivot configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
transformer = DataTransformer(
    index='date',
    columns=['platform', 'channel', 'page'],
    values='sessions'
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DETECTOR: Anomaly detection (80% confidence interval)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
detector = ForecastActualAnomalyDetector(
    transformer=transformer,
    dimension_names=['platform', 'channel', 'page'],
    lower_quantile_idx=1,   # q10
    upper_quantile_idx=9    # q90
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORMERS: Apply at different stages
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Stage 1: Filter forecast data (before detection)
# Remove low-traffic metrics to reduce noise
cumulative_filter = CumulativeThresholdFilter(
    transformer=transformer,
    threshold_pct=0.90  # Keep top 90% by forecast volume
)

# Stage 2: Filter actual data (before detection)
# Remove metrics with very low traffic
min_traffic_filter = ValueFilter(
    column='sessions',
    min_value=500  # At least 500 sessions
)

# Stage 3: Filter results (after detection)
# Only keep significant anomalies
anomaly_filter = ValueFilter(
    column='status',
    values=['BELOW_LOWER', 'ABOVE_UPPER'],
    mode='include'
)

deviation_filter = ValueFilter(
    column='deviation_pct',
    min_value=10.0  # At least 10% deviation
)

# Stage 4: Format results (before write)
formatter = ColumnFormatter.percentage(
    columns='deviation_pct',
    decimal_places=1
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WRITER: Save anomalies
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
writer = SQLiteDataWriter(
    db_path='/tmp/anomalies.db',
    table_name='detected_anomalies'
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WORKFLOW: Assemble pipeline with transformers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
workflow = AnomalyDetectionWorkflow(
    forecast_reader=forecast_reader,
    actual_reader=actual_reader,
    anomaly_detector=detector,
    data_writer=writer,
    transformers={
        'after_forecast_read': [
            cumulative_filter  # Remove low-traffic metrics
        ],
        'after_actual_read': [
            min_traffic_filter  # Data quality filter
        ],
        'after_detection': [
            anomaly_filter,     # Only anomalies
            deviation_filter    # Significant deviations only
        ],
        'before_write': [
            formatter          # Format as percentage
        ]
    }
)

# Execute pipeline
anomalies = workflow.run()

print(f"\\nâœ“ Pipeline complete!")
print(f"  Input forecasts: {len(forecast_reader.read())}")
print(f"  Input actuals: {len(actual_reader.read())}")
print(f"  After filters: ~{len(anomalies)}")
print(f"  Significant anomalies: {len(anomalies)}")
print(f"  Saved to: /tmp/anomalies.db")
'''

print(workflow_code)

# ============================================================================
# STEP 3: Pipeline flow
# ============================================================================

print("\n\nðŸ“Š PIPELINE FLOW")
print("=" * 70)
print("""
Step 1: READ FORECAST
  Source: /tmp/forecasts.db
  Query:  SELECT with date filter
  Output: 5 forecast rows with quantiles

Step 2: TRANSFORM (after_forecast_read)
  Filter: CumulativeThresholdFilter (top 90%)
  Remove: Low-traffic metrics (tablet_organic_blog)
  Output: 4 high-value metrics

Step 3: READ ACTUAL
  Source: /tmp/actuals.db
  Query:  SELECT with date filter
  Output: 5 actual measurements

Step 4: TRANSFORM (after_actual_read)
  Filter: ValueFilter (min_value=500)
  Remove: Very low traffic metrics
  Output: 4 metrics with sufficient traffic

Step 5: DETECT ANOMALIES
  Input:  4 forecast + 4 actual (matched)
  Method: Compare actual vs forecast quantiles (q10/q90)
  Output: 4 detection results (IN_RANGE, BELOW_LOWER, ABOVE_UPPER)

Step 6: TRANSFORM (after_detection)
  Filter: ValueFilter (status) â†’ Only anomalies
  Filter: ValueFilter (deviation) â†’ Only >10% deviation
  Output: 3 significant anomalies

Step 7: TRANSFORM (before_write)
  Format: ColumnFormatter.percentage()
  Output: "15.0%" instead of 15.0

Step 8: WRITE
  Target: /tmp/anomalies.db
  Table:  detected_anomalies
  Rows:   3 significant anomalies
""")

# ============================================================================
# STEP 4: Expected results
# ============================================================================

print("\n\nðŸ“ˆ EXPECTED RESULTS")
print("=" * 70)

expected_results = pd.DataFrame({
    'date': [datetime(2024, 11, 10)] * 3,
    'platform': ['mobile', 'desktop', 'mobile'],
    'channel': ['paid', 'paid', 'organic'],
    'page': ['product', 'product', 'home'],
    'actual': [3500, 1700, 3200],
    'forecast': [3000, 2000, 4000],
    'lower_bound': [2700, 1800, 3600],
    'upper_bound': [3300, 2200, 4400],
    'status': ['ABOVE_UPPER', 'BELOW_LOWER', 'BELOW_LOWER'],
    'deviation_pct': ['16.7%', '15.0%', '20.0%']
})

print(expected_results.to_string(index=False))

# ============================================================================
# STEP 5: Benefits of transformer stages
# ============================================================================

print("\n\nðŸ’¡ WHY USE MULTI-STAGE TRANSFORMERS?")
print("=" * 70)
print("""
1. EFFICIENCY (after_forecast_read, after_actual_read)
   âœ“ Filter data BEFORE detection (less computation)
   âœ“ Remove low-quality data early
   âœ“ Reduce memory usage
   Example: Filter out metrics with <500 sessions

2. DATA QUALITY (after_forecast_read, after_actual_read)
   âœ“ Remove outliers before detection
   âœ“ Handle missing data
   âœ“ Normalize units
   Example: Remove negative values, NULL checks

3. FOCUS (after_detection)
   âœ“ Keep only actionable anomalies
   âœ“ Filter by severity
   âœ“ Remove false positives
   Example: Only >10% deviation

4. PRESENTATION (before_write)
   âœ“ Format for reporting
   âœ“ Add computed columns
   âœ“ Round numbers
   Example: Format 15.3 â†’ "15.3%"

5. COMPOSABILITY
   âœ“ Mix and match transformers
   âœ“ Add/remove without changing code structure
   âœ“ Test transformers independently
   âœ“ Reuse across different pipelines
""")

# ============================================================================
# STEP 6: Advanced transformer patterns
# ============================================================================

print("\n\nðŸ”§ ADVANCED TRANSFORMER PATTERNS")
print("=" * 70)

advanced_patterns = '''
# Pattern 1: Time-based filtering
workflow = AnomalyDetectionWorkflow(
    ...,
    transformers={
        'after_forecast_read': [
            ValueFilter('date', values=pd.date_range('2024-11-01', '2024-11-30'))
        ]
    }
)

# Pattern 2: Dimension filtering
workflow = AnomalyDetectionWorkflow(
    ...,
    transformers={
        'after_actual_read': [
            ValueFilter('platform', values=['desktop', 'mobile']),  # Exclude tablet
            ValueFilter('channel', values=['spam'], mode='exclude')  # Exclude spam
        ]
    }
)

# Pattern 3: Multi-tier alerting
critical_workflow = AnomalyDetectionWorkflow(
    ...,
    transformers={
        'after_detection': [
            ValueFilter('status', values=['BELOW_LOWER', 'ABOVE_UPPER']),
            ValueFilter('deviation_pct', min_value=50.0),  # Critical: >50%
            ColumnFormatter.percentage('deviation_pct')
        ]
    }
)

warning_workflow = AnomalyDetectionWorkflow(
    ...,
    transformers={
        'after_detection': [
            ValueFilter('status', values=['BELOW_LOWER', 'ABOVE_UPPER']),
            ValueFilter('deviation_pct', min_value=20.0, max_value=50.0),  # Warning: 20-50%
            ColumnFormatter.percentage('deviation_pct')
        ]
    }
)

# Pattern 4: Custom formatting
workflow = AnomalyDetectionWorkflow(
    ...,
    transformers={
        'before_write': [
            ColumnFormatter({
                'actual': lambda x: f"{x:,.0f}",           # 5000 â†’ "5,000"
                'forecast': lambda x: f"{x:,.0f}",         # 5000 â†’ "5,000"
                'deviation_pct': lambda x: f"{x:.1f}%",    # 15.3 â†’ "15.3%"
                'status': lambda x: 'ðŸ”´' if x == 'ABOVE_UPPER' else 'ðŸ”µ' if x == 'BELOW_LOWER' else 'ðŸŸ¢'
            })
        ]
    }
)
'''

print(advanced_patterns)

# ============================================================================
# STEP 7: Production tips
# ============================================================================

print("\n\nðŸš€ PRODUCTION TIPS")
print("=" * 70)
print("""
1. DATABASE OPTIMIZATION
   âœ“ Create indexes on date, platform, channel columns
   âœ“ Use appropriate data types (INTEGER, REAL, TEXT, DATE)
   âœ“ Vacuum database regularly
   âœ“ Monitor database size

2. ERROR HANDLING
   âœ“ Handle missing forecasts gracefully
   âœ“ Validate data quality before detection
   âœ“ Log transformer actions for debugging
   âœ“ Set up alerts for failed pipelines

3. SCHEDULING
   âœ“ Run after forecast workflow completes
   âœ“ Use cron or Airflow for scheduling
   âœ“ Typical schedule: 3:00 AM daily
   âœ“ Monitor execution time

4. ALERTING
   âœ“ Send notifications for critical anomalies
   âœ“ Include context (metric, deviation, trend)
   âœ“ Avoid alert fatigue (use severity levels)
   âœ“ Provide actionable recommendations

5. MONITORING
   âœ“ Track detection rate (% of metrics with anomalies)
   âœ“ Monitor false positive rate
   âœ“ Log transformer effectiveness
   âœ“ Review and tune thresholds monthly
""")

print("\n" + "=" * 70)
print("This example shows the POWER of multi-stage transformers!")
print("Filter â†’ Detect â†’ Format â†’ Alert for production-ready anomaly detection.")
print("=" * 70)
